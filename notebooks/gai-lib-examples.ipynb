{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Lib: Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Text (TTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text generation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mistral-7B\n",
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who had spent her entire life tending to her garden. She was known throughout the land for her beautiful flowers and lush vegetables, which she would sell at the local market every week. One day, as she was out picking her produce, she stumbled across a mysterious seed that she had never seen before. Without hesitation, she planted it in her garden and watched as it grew into the most magnificent tree she had ever seen. The tree bore fruit unlike any other, with juicy oranges that were filled with gold coins. The villagers were amazed by this newfound treasure and flocked to the old woman's house to get their share. From then on, the old woman became rich beyond her wildest dreams, all thanks to the magical seed she found in her garden.({'finish_reason': 'stop'}, 'finish_reason')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# Mistral7B\n",
    "print(\"> Mistral-7B\")\n",
    "for chunk in ggg(category=\"ttt\",messages=\"user: Tell me a one paragraph story\\nassistant:\"):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> OpenAI\n",
      "In the heart of a bustling metropolis, there lived an odd-job man named Samuel who was notorious for his unfailing cheerfulness in the face of relentless hard work. One ordinary day, while he was hauling discarded artifacts from a nearby thrift shop, he chanced upon an ancient-looking lamp caked with years of dirt and neglect. Hoping to sell it for a few extra bucks, Samuel scrubbed the lamp clean and, to his stunned surprise, a vibrant genie materialized amidst a swirl of multicoloured mist! The genie granted him three wishes, but Samuel - forever content with what he had - humbly wished for happiness for everyone else, the welfare of strays and a beautiful home. His wishes granted, Samuel continued to live his life in the same way, yet left an indelible mark by unknowingly becoming the epicenter of happiness in his city.None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# GPT4\n",
    "print(\"> OpenAI\")\n",
    "for chunk in ggg(category=\"ttt\",generator=\"gpt-4\",messages=\"user: Tell me a one paragraph story\\nassistant:\"):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> OpenAI API\n",
      "Once upon a time in the heartland of Asia, was a brave yet kind-hearted monk named Zhuang. He resided in an ancient, serene monastery perched at the peak of the snow-covered Himalayas. One dreadfully cold winter, a fierce storm enveloped the region, burying surrounding villages in a relentless blanket of snow. Families were trapped and food quickly dwindled. As despair set in, a figure emerged from the storm's fury, Zhuang, bearing food and medicine from the"
     ]
    }
   ],
   "source": [
    "print(\"> OpenAI API\")\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise Exception(\n",
    "        \"OPENAI_API_KEY not found in environment variables\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"}],\n",
    "    stream=True,\n",
    "    max_tokens=100,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTT With Function Call\n",
    "\n",
    "OpenAPI provides a powerful feature for its API known as Function Calling. Essentially, this is a mechanism for the LLM to seek external assistance when it encounters limitations in its text generation capabilities. It does this by returning a string that emulates the calling of a function, based on the function description provided by the user.\n",
    "\n",
    "In the following example, we demonstrate function calling  being to an open source model using Mistral7b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mistral-7B\n",
      "{'type': 'function', 'name': 'gg', 'arguments': ' { \"search_query\": \"current date\" }'}\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# Mistral7B\n",
    "print(\"> Mistral-7B\")\n",
    "response = ggg(category=\"ttt\",\n",
    "    messages=\"user: What is today's date?\\nassistant:\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }                   \n",
    "    ],\n",
    "    stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> OpenAI\n",
      "{'type': 'function', 'name': 'gg', 'arguments': '{\\n  \"search_query\": \"current president of Singapore\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# OpenAI\n",
    "print(\"> OpenAI\")\n",
    "response = ggg(category=\"ttt\",\n",
    "                generator=\"gpt-4\",\n",
    "               messages=\"user: Who is the current president of Singapore?\\nassistant:\",\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"gg\",\n",
    "                            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                            \"parameters\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"search_query\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"search_query\"]\n",
    "                            }\n",
    "                        }\n",
    "                    }                   \n",
    "                ],\n",
    "               stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> OpenAI Original\n",
      "{\n",
      "    \"arguments\": \"{\\n  \\\"search_query\\\": \\\"latest news Singapore\\\"\\n}\",\n",
      "    \"name\": \"gg\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"> OpenAI Original\")\n",
    "import os,json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise Exception(\n",
    "        \"OPENAI_API_KEY not found in environment variables\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me the latest news on Singapore\"}],\n",
    "    stream=True,\n",
    "    max_tokens=100,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }                   \n",
    "    ],\n",
    ")\n",
    "tool = {}\n",
    "tool[\"arguments\"]=\"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.tool_calls and chunk.choices[0].delta.tool_calls[0].function.name:\n",
    "        tool[\"name\"] = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "    elif chunk.choices[0].delta.tool_calls and chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "        tool[\"arguments\"] += chunk.choices[0].delta.tool_calls[0].function.arguments\n",
    "print(json.dumps(tool, indent=4)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech (TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.common.sound_utils import play_audio,save_audio\n",
    "\n",
    "data = {\n",
    "    \"input\": \"The definition of insanity is doing the same thing over and over and expecting different results.\",\n",
    "    \"voice\": None,\n",
    "    \"language\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "response = ggg(\"tts\", **data)\n",
    "play_audio(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai tts\n",
    "response = ggg(\"tts\", generator=\"openai-tts-1\", **data)\n",
    "play_audio(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai original\n",
    "response = client.audio.speech.create(\n",
    "    model='tts-1', input=\"The definition of insanity is doing the same thing over and over and expecting different results.\", voice=\"alloy\")\n",
    "play_audio(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text (STT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "with open(\"../gai-lib/tests/lib/stt/today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    play_audio(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# OpenSource Whisper\n",
    "with open(\"../gai-lib/tests/lib/stt/today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    output = ggg(\"stt\", file=f)\n",
    "    print(output.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Whisper\n",
    "with open(\"../gai-lib/tests/lib/stt/today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    output = ggg(\"stt\", generator=\"openai-whisper\", file=f)\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Text (ITT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.common.image_utils import read_to_base64\n",
    "import os\n",
    "from IPython.display import Image,display\n",
    "image_file = os.path.join(\"../gai-lib/tests/lib/itt\", \"buses.jpeg\")\n",
    "encoded_string = read_to_base64(image_file)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "display(Image(image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llava\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "print(\"> Llava\")\n",
    "for chunk in ggg(\"itt\",messages=messages,stream=True):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "print(\"> OpenAI\")\n",
    "for chunk in ggg(category=\"itt\", generator=\"openai-vision\", messages=messages, stream=True, max_tokens=100):\n",
    "    print(chunk.decode(), end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the progress of the indexing status of the remote service by running the following:\n",
    "\n",
    "a) /tests/client/rag/function_test_websocket_forwarder.py\n",
    "\n",
    "b) /tests/client/rag/function_test_websocket_forwarder_listener.py\n",
    "\n",
    "The first script will forward the status of the remote service to the local port.\n",
    "The second script will pull the status from the local port and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 10:14:39 INFO gai.lib.RAGClient:\u001b[32mRAGClient.delete_collection: Deleting collection https://gaiaio.ai/api/gen/v1/rag/collection/demo\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete \"demo\" Collection.\n",
    "from gai.lib.RAGClient import RAGClient\n",
    "rag = RAGClient(\"../gai-lib/gai.yml\")\n",
    "rag.delete_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collections': []}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Collections\n",
    "from gai.lib.RAGClient import RAGClient\n",
    "rag = RAGClient(\"../gai-lib/gai.yml\")\n",
    "rag.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Listener\n",
    "\n",
    "Before continuing with the following steps, start an external rag_listener to monitor the progress of the indexing status.\n",
    "\n",
    "```bash\n",
    "cd /gai-lib/tests/client/rag\n",
    "python rag_listener.py\n",
    "```\n",
    "\n",
    "#### Start Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': '83caa783-bbfb-46e5-b781-14d14936b92f'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index 2023 national day speech \n",
    "from gai.lib.GGG import GGG\n",
    "ggg = GGG(\"../gai-lib/gai.yml\")\n",
    "def updater(status):\n",
    "    print(status)\n",
    "\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"file_path\": \"../gai-lib/tests/clients/rag/pm_long_speech_2023.txt\",\n",
    "    \"metadata\": {\"title\": \"2023 National Day Rally Speech\", \n",
    "    \"source\": \"https://www.pmo.gov.sg/Newsroom/national-day-rally-2023\"},\n",
    "}\n",
    "ggg(\"index\", **data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List documents\n",
    "from gai.lib.RAGClient import RAGClient\n",
    "rag = RAGClient(\"../gai-lib/gai.yml\")\n",
    "docs=rag.list_documents(\"demo\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document\n",
    "from gai.lib.RAGClient import RAGClient\n",
    "rag = RAGClient(\"../gai-lib/gai.yml\")\n",
    "rag.get_document(docs['documents'][0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg = GGG((\"../gai-lib/gai.yml\"))\n",
    "\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": \"Who are the young seniors?\",\n",
    "}\n",
    "response = ggg(\"retrieve\", **data)\n",
    "context = response.text\n",
    "question = \"Who are the young seniors?\"\n",
    "answer = ggg(\"ttt\", messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\")\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG((\"../gai-lib/gai.yml\"))\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag\",\n",
    "            \"description\": \"The 'rag' function is a specialized tool that allows the AI to perform semantic searches on PM Lee Hsien Loong's 2023 National Day Rally. It can be invoked when the AI needs to retrieve facts or information from the speech. This function utilizes advanced Natural Language Processing (NLP) techniques to understand and match the semantic meaning of the user's query with the content of the speech. This is particularly useful when the user's query relates to specific themes, topics, or statements made during the rally.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"[query_1, query_2, query_3]\",\n",
    "                        \"description\": \"An array of search queries to perform a semantic search in the vector database. Each string in the array represents a different way of asking the question. This expands the coverage of the search and increases the chance of finding the best match. For example, instead of using one query like 'economic policies', use multiple variations like ['PM Lee Hsien Loong's economic policies announced at the 2023 National Day Rally', 'What were the economic strategies discussed by PM Lee in 2023 National Day Rally?', 'Economic measures announced by PM Lee in 2023 Rally'].\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# RAG + Function Call\n",
    "import json\n",
    "print(\"> Mistral-7B\")\n",
    "question = \"What did PM Lee say about young seniors?\"\n",
    "\n",
    "messages = [{'role':'user','content':question},{'role':'assistant','content':''}]\n",
    "response = ggg(category=\"ttt\",\n",
    "               messages=messages, \n",
    "               tools=tools,\n",
    "               stream=False,\n",
    "               max_new_tokens=100)\n",
    "result=response.decode()\n",
    "search_query = json.loads(result['arguments'])['search_query'][0]\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": search_query,\n",
    "}\n",
    "response = ggg(\"retrieve\", **data)\n",
    "context = response.text\n",
    "answer = ggg(\"ttt\", messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\")\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete document\n",
    "from gai.lib.RAGClient import RAGClient\n",
    "rag = RAGClient((\"../gai-lib/gai.yml\"))\n",
    "rag.delete_document(docs['documents'][0]['id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai-lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
