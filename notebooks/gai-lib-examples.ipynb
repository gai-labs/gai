{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Lib: Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Text (TTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Mistral7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# Mistral7B\n",
    "print(\"> Mistral-7B\")\n",
    "for chunk in ggg(category=\"ttt\",messages=\"user: Tell me a one paragraph story\\nassistant:\"):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# GPT4\n",
    "print(\"> OpenAI\")\n",
    "for chunk in ggg(category=\"ttt\",generator=\"gpt-4\",messages=\"user: Tell me a one paragraph story\\nassistant:\"):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: API Compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"> OpenAI API\")\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise Exception(\n",
    "        \"OPENAI_API_KEY not found in environment variables\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"}],\n",
    "    stream=True,\n",
    "    max_tokens=100,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTT With Function Call\n",
    "\n",
    "OpenAPI provides a powerful feature for its API known as Function Calling. Essentially, this is a mechanism for the LLM to seek external assistance when it encounters limitations in its text generation capabilities. It does this by returning a string that emulates the calling of a function, based on the function description provided by the user.\n",
    "\n",
    "In the following example, we demonstrate function calling  being to an open source model using Mistral7b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Function call with Mistral7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# Mistral7B\n",
    "print(\"> Mistral-7B\")\n",
    "response = ggg(category=\"ttt\",\n",
    "    messages=\"user: What is today's date?\\nassistant:\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }                   \n",
    "    ],\n",
    "    stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: function call with GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# OpenAI\n",
    "print(\"> OpenAI\")\n",
    "response = ggg(category=\"ttt\",\n",
    "                generator=\"gpt-4\",\n",
    "               messages=\"user: Who is the current president of Singapore?\\nassistant:\",\n",
    "                tools=[\n",
    "                    {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"gg\",\n",
    "                            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                            \"parameters\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"search_query\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"search_query\"]\n",
    "                            }\n",
    "                        }\n",
    "                    }                   \n",
    "                ],\n",
    "               stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: function call with API compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"> OpenAI Original\")\n",
    "import os,json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise Exception(\n",
    "        \"OPENAI_API_KEY not found in environment variables\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me the latest news on Singapore\"}],\n",
    "    stream=True,\n",
    "    max_tokens=100,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }                   \n",
    "    ],\n",
    ")\n",
    "tool = {}\n",
    "tool[\"arguments\"]=\"\"\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.tool_calls and chunk.choices[0].delta.tool_calls[0].function.name:\n",
    "        tool[\"name\"] = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "    elif chunk.choices[0].delta.tool_calls and chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "        tool[\"arguments\"] += chunk.choices[0].delta.tool_calls[0].function.arguments\n",
    "print(json.dumps(tool, indent=4)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech (TTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the input text \"The definition of insanity is doing the same thing over and over and expecting different results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.common.sound_utils import play_audio,save_audio\n",
    "\n",
    "data = {\n",
    "    \"input\": \"The definition of insanity is doing the same thing over and over and expecting different results.\",\n",
    "    \"voice\": None,\n",
    "    \"language\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Generate speech with Coqui xTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 10:32:32 DEBUG gai.lib.TTSClient:\u001b[35mbase_url=https://gaiaio.ai/api/gen\u001b[0m\n",
      "2024-02-24 10:32:32 DEBUG gai.common.http_utils:\u001b[35mhttppost:url=https://gaiaio.ai/api/gen/v1/audio/speech\u001b[0m\n",
      "2024-02-24 10:32:32 DEBUG gai.common.http_utils:\u001b[35mhttppost:data={'input': 'The definition of insanity is doing the same thing over and over '\n",
      "          'and expecting different results.',\n",
      " 'language': None,\n",
      " 'model': 'xtts-2',\n",
      " 'stream': True,\n",
      " 'voice': None}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "response = ggg(\"tts\", **data)\n",
    "play_audio(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Generate speech with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai tts\n",
    "response = ggg(\"tts\", generator=\"openai-tts-1\", **data)\n",
    "play_audio(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: API Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai original\n",
    "response = client.audio.speech.create(\n",
    "    model='tts-1', input=\"The definition of insanity is doing the same thing over and over and expecting different results.\", voice=\"alloy\")\n",
    "play_audio(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text (STT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "with open(\"./today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    play_audio(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Transcribe audio with local Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "# OpenSource Whisper\n",
    "with open(\"./today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    output = ggg(\"stt\", file=f)\n",
    "    print(output.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Transcribe audio with OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Whisper\n",
    "with open(\"./today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    output = ggg(\"stt\", generator=\"openai-whisper\", file=f)\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Text (ITT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.common.image_utils import read_to_base64\n",
    "import os\n",
    "from IPython.display import Image,display\n",
    "encoded_string = read_to_base64(\"./buses.jpeg\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "display(Image(\"./buses.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Describe with Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llava\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"../gai-lib/gai.yml\")\n",
    "\n",
    "print(\"> Llava\")\n",
    "for chunk in ggg(\"itt\",messages=messages,stream=True):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Describe with OpenAI Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "print(\"> OpenAI\")\n",
    "for chunk in ggg(category=\"itt\", generator=\"openai-vision\", messages=messages, stream=True, max_tokens=100):\n",
    "    print(chunk.decode(), end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the progress of the indexing status of the remote service by running the following:\n",
    "\n",
    "a) /tests/client/rag/function_test_websocket_forwarder.py\n",
    "\n",
    "b) /tests/client/rag/function_test_websocket_forwarder_listener.py\n",
    "\n",
    "The first script will forward the status of the remote service to the local port.\n",
    "The second script will pull the status from the local port and display it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Start Listener\n",
    "\n",
    "Before continuing with the following steps, start an external rag_listener to monitor the progress of the indexing status.\n",
    "\n",
    "```sh\n",
    "cd /gai-lib/tests/clients/rag\n",
    "python rag_listener.py      \n",
    "```\n",
    "\n",
    "It should show **Connected to wss://gaiaio.ai/api/gen/v1/rag/ws**\n",
    "\n",
    "### Step 2: Start Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 2023 national day speech \n",
    "from gai.lib.GGG import GGG\n",
    "ggg = GGG(\"../gai-lib/gai.yml\")\n",
    "def updater(status):\n",
    "    print(status)\n",
    "\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"file_path\": \"../gai-lib/tests/clients/rag/pm_long_speech_2023.txt\",\n",
    "    \"metadata\": {\"title\": \"2023 National Day Rally Speech\", \n",
    "    \"source\": \"https://www.pmo.gov.sg/Newsroom/national-day-rally-2023\"},\n",
    "}\n",
    "ggg(\"index\", **data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listener output should show:\n",
    "\n",
    "```sh\n",
    "Callback: message={\"progress\":3}\n",
    "Received status update: {\"progress\":3}\n",
    "Callback: message={\"progress\":6}\n",
    "Received status update: {\"progress\":6}\n",
    "Callback: message={\"progress\":10}\n",
    "Received status update: {\"progress\":10}\n",
    "Callback: message={\"progress\":13}\n",
    "Received status update: {\"progress\":13}\n",
    "Callback: message={\"progress\":17}\n",
    "Received status update: {\"progress\":17}\n",
    "Callback: message={\"progress\":20}\n",
    "Received status update: {\"progress\":20}\n",
    "Callback: message={\"progress\":24}\n",
    "Received status update: {\"progress\":24}\n",
    "Callback: message={\"progress\":27}\n",
    "Received status update: {\"progress\":27}\n",
    "Callback: message={\"progress\":31}\n",
    "Received status update: {\"progress\":31}\n",
    "Callback: message={\"progress\":34}\n",
    "Received status update: {\"progress\":34}\n",
    "Callback: message={\"progress\":37}\n",
    "Received status update: {\"progress\":37}\n",
    "Callback: message={\"progress\":41}\n",
    "Received status update: {\"progress\":41}\n",
    "Callback: message={\"progress\":44}\n",
    "Received status update: {\"progress\":44}\n",
    "Callback: message={\"progress\":48}\n",
    "Received status update: {\"progress\":48}\n",
    "Callback: message={\"progress\":51}\n",
    "Received status update: {\"progress\":51}\n",
    "Callback: message={\"progress\":55}\n",
    "Received status update: {\"progress\":55}\n",
    "Callback: message={\"progress\":58}\n",
    "Received status update: {\"progress\":58}\n",
    "Callback: message={\"progress\":62}\n",
    "Received status update: {\"progress\":62}\n",
    "Callback: message={\"progress\":65}\n",
    "Received status update: {\"progress\":65}\n",
    "Callback: message={\"progress\":68}\n",
    "Received status update: {\"progress\":68}\n",
    "Callback: message={\"progress\":72}\n",
    "Received status update: {\"progress\":72}\n",
    "Callback: message={\"progress\":75}\n",
    "Received status update: {\"progress\":75}\n",
    "Callback: message={\"progress\":79}\n",
    "Received status update: {\"progress\":79}\n",
    "Callback: message={\"progress\":82}\n",
    "Received status update: {\"progress\":82}\n",
    "Callback: message={\"progress\":86}\n",
    "Received status update: {\"progress\":86}\n",
    "Callback: message={\"progress\":89}\n",
    "Received status update: {\"progress\":89}\n",
    "Callback: message={\"progress\":93}\n",
    "Received status update: {\"progress\":93}\n",
    "Callback: message={\"progress\":96}\n",
    "Received status update: {\"progress\":96}\n",
    "Callback: message={\"progress\":100}\n",
    "Received status update: {\"progress\":100}\n",
    "Callback: message=<stop>\n",
    "Received status update: <stop>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Query and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg = GGG((\"../gai-lib/gai.yml\"))\n",
    "\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": \"Who are the young seniors?\",\n",
    "}\n",
    "response = ggg(\"retrieve\", **data)\n",
    "context = response.text\n",
    "question = \"Who are the young seniors?\"\n",
    "answer = ggg(\"ttt\", messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\")\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query and Retrieve with Function Call\n",
    "\n",
    "The AI will decide based on the context of the conversation, if the response require retrieval to answer the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG((\"../gai-lib/gai.yml\"))\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag\",\n",
    "            \"description\": \"The 'rag' function is a specialized tool that allows the AI to perform semantic searches on PM Lee Hsien Loong's 2023 National Day Rally. It can be invoked when the AI needs to retrieve facts or information from the speech. This function utilizes advanced Natural Language Processing (NLP) techniques to understand and match the semantic meaning of the user's query with the content of the speech. This is particularly useful when the user's query relates to specific themes, topics, or statements made during the rally.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"[query_1, query_2, query_3]\",\n",
    "                        \"description\": \"An array of search queries to perform a semantic search in the vector database. Each string in the array represents a different way of asking the question. This expands the coverage of the search and increases the chance of finding the best match. For example, instead of using one query like 'economic policies', use multiple variations like ['PM Lee Hsien Loong's economic policies announced at the 2023 National Day Rally', 'What were the economic strategies discussed by PM Lee in 2023 National Day Rally?', 'Economic measures announced by PM Lee in 2023 Rally'].\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# RAG + Function Call\n",
    "import json\n",
    "print(\"> Mistral-7B\")\n",
    "question = \"What did PM Lee say about young seniors?\"\n",
    "\n",
    "messages = [{'role':'user','content':question},{'role':'assistant','content':''}]\n",
    "response = ggg(category=\"ttt\",\n",
    "               messages=messages, \n",
    "               tools=tools,\n",
    "               stream=False,\n",
    "               max_new_tokens=100)\n",
    "result=response.decode()\n",
    "search_query = json.loads(result['arguments'])['search_query'][0]\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": search_query,\n",
    "}\n",
    "response = ggg(\"retrieve\", **data)\n",
    "context = response.text\n",
    "answer = ggg(\"ttt\", messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\")\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai-lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
