{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install -e \".[TTT]\"\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT4\n",
    "\n",
    "For (1) and (2) below, you will use the GaiGen library to call OpenAI's GPT4.\n",
    "You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=100,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 8k-context 4-bit quantized\n",
    "\n",
    "For (3) and (4), you will run Mistral 7B locally. Clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance. Make sure you have huggingface-hub installed, if not run `pip install huggingface-hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        config.json\n",
    "        model.safetensors \n",
    "        tokenizer.model\n",
    "        --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100, stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarn-Mistral-7B 128k-context 4-bit quantized\n",
    "\n",
    "Repeat the earlier examples but using a different version of Mistral-7B model with a larger context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir ~/gai/models/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their paper, the perplexity seems better than the original once the token length is greater than 10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity-of-mistral7b-128k](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropics Claude2.1\n",
    "\n",
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B with HuggingFace transformers\n",
    "\n",
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B GGUF with LlaMaCPP (CPU only)\n",
    "\n",
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the model\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \\\n",
    "                config.json \\\n",
    "                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "with io.capture_output() as captured:\n",
    "    # Redirect stderr to stdout\n",
    "    sys.stderr = sys.stdout    \n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using Function Call\n",
    "\n",
    "OpenAPI provided a powerful feature for its API called Function calling. It is essentially a way for the LLM to seek external help when encountering limitation to its ability to generate text but returning a string emulating the calling of a function based on the function description provied by the user.\n",
    "\n",
    "We will create a set of tools that can be made available to the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will show how it works in gpt-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model decided to use tool: \n",
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_exlvhtAlutM2Ueegg89724lU', function=Function(arguments='{\\n  \"search_query\": \"PM Lee Hsien Loong 2023 national day rally location\"\\n}', name='gg'), type='function')])\n",
      "Model decided not to use tool: \n",
      "ChatCompletionMessage(content='Once upon a time, in a small town nestled between a sparkling river and a towering mountain, lived a young girl named Lily. She was known throughout the town for her radiant smile and immense kindness. One day, she discovered a magical stone that could grant one wish to the one who found it. Instead of wishing for wealth or fame, Lily wished for the happiness of everyone in her town. The next day, the town was filled with joyous laughter and cheer, everyone was happy. That', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "\n",
    "print(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "print(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we demonstrate the same feature applied to Mistral-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model decided to use tool: \n",
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_daf88dac-369c-498f-bd11-0801566a51df', function=Function(arguments='{\"search_query\": \"PM Lee Hsien Loong 2023 National Day Rally location\"}', name='gg'), type='function')])\n",
      "Model decided not to use tool: \n",
      "ChatCompletionMessage(content=' Once upon a time, there was a little girl named Alice who lived in an enchanted forest full of talking animals and magical creatures. One day, she stumbled upon a secret door hidden behind a waterfall and decided to explore it. As she entered through the door, she found herself in a beautiful kingdom ruled by a kind queen. The queen welcomed her warmly and showed her around the castle, introducing her to all sorts of fascinating people and things. Alice had so much fun that she never wanted to leave!', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "\n",
    "print(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "print(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop any container with the same name\n",
    "docker rm -f gai-ttt\n",
    "\n",
    "# Start the container\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "\n",
    "# Wait for model to load\n",
    "sleep 30\n",
    "\n",
    "# Confirm its running\n",
    "docker logs gai-ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 2: Run Text Generation Client\n",
    "\n",
    "The default model is Mistral7B-8k context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a one paragraph short story.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True)\n",
    "for chunk in response.iter_lines():\n",
    "    result = json.loads(chunk.decode('utf-8'))\n",
    "    print(result[\"choices\"][0][\"delta\"][\"content\"],end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-53e917e4-2703-4587-bede-5a5de96f12f3\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":[{\"id\":\"call_fb4edf0a-a509-4c24-91a3-78dd8b4fdcf5\",\"function\":{\"arguments\":\"{\\\"search_query\\\": \\\"current date\\\"}\",\"name\":\"gg\"},\"type\":\"function\"}]}}],\"created\":1707944860,\"model\":\"Mistral7B-ExLlama\",\"object\":\"chat.completion\",\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":3620,\"total_tokens\":3620}}\n"
     ]
    }
   ],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is today's date?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": False,\n",
    "        \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    stream=False)\n",
    "\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
