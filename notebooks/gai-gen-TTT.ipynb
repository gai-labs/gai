{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install -e \".[TTT]\"\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT4\n",
    "\n",
    "For (1) and (2) below, you will use the GaiGen library to call OpenAI's GPT4.\n",
    "You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:56:20 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator gpt-4...\u001b[0m\n",
      "2024-03-07 06:56:20 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine OpenAI_TTT...\u001b[0m\n",
      "2024-03-07 06:56:20 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of the ancient town of Littleshire, there lived a peculiar old man named Mr. Bentley. Known for his exceptional talent for fixing anything, he was the town's favorite handyman. One day, a mysterious object fell from the sky, alarming the whole town. It was a complex machine, unlike anything anyone had ever seen. Fearful townsfolk turned to Mr. Bentley, who, with his gentle touch and intuitive understanding of machines, fixed the strange contraption. The\n"
     ]
    }
   ],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=100,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:56:30 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-03-07 06:56:30 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      "In the heart of a bustling city, lived an old watchmaker named Gregory. Every day, he worked meticulously, piecing together the intricate mechanics of timepieces, his hands telling a story of countless years of precision and dedication. One night, a peculiar customer walked into his shop, presenting a centuries-old watch that had long stopped ticking. Gregory, intrigued and challenged, spent weeks bringing it back to life. As the timepiece began to tick again, it revealed a hidden compartment with an old map"
     ]
    }
   ],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 8k-context 4-bit quantized\n",
    "\n",
    "For (3) and (4), you will run Mistral 7B locally. Clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance. Make sure you have huggingface-hub installed, if not run `pip install huggingface-hub`.\n",
    "\n",
    "```sh\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        config.json\n",
    "        model.safetensors \n",
    "        tokenizer.model\n",
    "        --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:56:45 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: New generator_name specified, unload current generator.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:56:51 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator mistral7b-exllama...\u001b[0m\n",
      "2024-03-07 06:56:51 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine ExLlama_TTT...\u001b[0m\n",
      "2024-03-07 06:56:51 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/Mistral-7B-Instruct-v0.1-GPTQ\u001b[0m\n",
      "2024-03-07 06:56:51 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT.load: Loading model from /home/roylai/gai/models/Mistral-7B-Instruct-v0.1-GPTQ/model.safetensors\u001b[0m\n",
      "2024-03-07 06:57:16 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-03-07 06:57:16 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-03-07 06:57:16 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=16\u001b[0m\n",
      "2024-03-07 06:57:29 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by max_new_tokens: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she believed that everything happened for a reason. One day, as she sat on her porch watching the sun set over the mountains, she noticed a young boy playing in the field across the street\n"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100, stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:58:11 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-03-07 06:58:11 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-03-07 06:58:11 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-03-07 06:58:12 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and sharing her knowledge with others. One day, as she sat by the river, a young boy approached her with a troubled expression on his face. He told her that he"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 06:58:18 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by max_new_tokens: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " didn't know what to do with his life"
     ]
    }
   ],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarn-Mistral-7B 128k-context 4-bit quantized\n",
    "\n",
    "Repeat the earlier examples but using a different version of Mistral-7B model with a larger context window.\n",
    "\n",
    "```sh\n",
    "huggingface-cli download TheBloke/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir ~/gai/models/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their paper, the perplexity seems better than the original once the token length is greater than 10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity-of-mistral7b-128k](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:01:06 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator mistral7b_128k-exllama...\u001b[0m\n",
      "2024-03-07 07:01:06 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine ExLlama_TTT...\u001b[0m\n",
      "2024-03-07 07:01:06 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/Yarn-Mistral-7B-128k-GPTQ\u001b[0m\n",
      "2024-03-07 07:01:06 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT.load: Loading model from /home/roylai/gai/models/Yarn-Mistral-7B-128k-GPTQ/model.safetensors\u001b[0m\n",
      "2024-03-07 07:01:23 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-03-07 07:01:23 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-03-07 07:01:23 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=16\u001b[0m\n",
      "2024-03-07 07:02:20 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by max_new_tokens: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happier she became until finally she sat down under its"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:02:35 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-03-07 07:02:35 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-03-07 07:02:35 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-03-07 07:02:35 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      " Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:02:58 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by max_new_tokens: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ier she became until finally she sat down under its"
     ]
    }
   ],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropics Claude2.1\n",
    "\n",
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B with HuggingFace transformers\n",
    "\n",
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B GGUF with LlaMaCPP (CPU only)\n",
    "\n",
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the model\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \\\n",
    "                config.json \\\n",
    "                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "with io.capture_output() as captured:\n",
    "    # Redirect stderr to stdout\n",
    "    sys.stderr = sys.stdout    \n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using Function Call\n",
    "\n",
    "OpenAPI provided a powerful feature for its API called Function calling. It is essentially a way for the LLM to seek external help when encountering limitation to its ability to generate text but returning a string emulating the calling of a function based on the function description provied by the user.\n",
    "\n",
    "We will create a set of tools that can be made available to the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will show how it works in gpt-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:08:19 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: New generator_name specified, unload current generator.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:08:19 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator gpt-4...\u001b[0m\n",
      "2024-03-07 07:08:19 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine OpenAI_TTT...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:08:19 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1, 'tools': [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_MRSjMb7JHsU9mu3D1MDE6atO', function=Function(arguments='{\\n  \"search_query\": \"PM Lee Hsien Loong 2023 national day rally location\"\\n}', name='gg'), type='function')])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided not to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:08:22 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1, 'tools': [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Once upon a time, in a small, peaceful village nestled between the lush green hills, lived a kind-hearted blacksmith named Albert. One day, he discovered a magical stone that could turn anything it touched into gold. Overwhelmed with joy, Albert turned everything he owned into gold, but soon realized his food also turned to gold, leaving him starving. In his despair, he pleaded with the stone to take back its magic. The stone, hearing his genuine remorse, reversed its spell.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we demonstrate the same feature applied to Mistral-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:07:19 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:07:19 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=system: system:\n",
      "\n",
      "\n",
      "                You will always begin your interaction by asking yourself if the user's message is a message that requires a tool response or a text response.\n",
      "                                \n",
      "                DEFINITIONS:\n",
      "                1. A tool response is based on the following JSON format:\n",
      "                        <tool>\n",
      "                        {\n",
      "                            'function': {\n",
      "                                'name': ...,\n",
      "                                'parameters': ...\n",
      "                            }\n",
      "                        }\n",
      "                        </tool>\n",
      "                \n",
      "                And the tool is chosen from the following <tools> list:\n",
      "                        <tools>\n",
      "                        [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]\n",
      "                        </tools>.\n",
      "                    \n",
      "                2. A text response is based on the following JSON format:\n",
      "                        <text>\n",
      "                        {\n",
      "                            'text': ...\n",
      "                        }\n",
      "                        </text>\n",
      "                \n",
      "                STEPS:\n",
      "                1. Think about the nature of the user's message.\n",
      "                    * Is the user's message a question that I can answer factually within my knowledge domain?\n",
      "                    * Are there any dependencies to external factors that I need to consider before answering the user's question?\n",
      "                    * What are the tools I have at my disposal to help me answer the user's question? \n",
      "                2. If the user's message requires a tool response, pick the most suitable tool response from <tools>. \n",
      "                    * I can refer to the \"description\" field of each tool to help me decide.\n",
      "                    * For example, if I need to search for real-time information, I can use the \"gg\" tool and if I know where to find the information, I can use the \"scrape\" tool.\n",
      "                3. If the user's message does not require a tool response, provide a text response to the user.\n",
      "\n",
      "                CONSTRAINTS:        \n",
      "                1. You can only provide a tool response or a text response and nothing else.\n",
      "                2. When providing a tool response, respond only in JSON and only pick from <tools>. That means, begin your message with a curly bracket ' and end your message with a curly bracket '. Do not respond with anything else.\n",
      "                3. Remember, do not invent your own tools. You can only pick from <tools>.\n",
      "user: Where did PM Lee Hsien Loong hold his 2023 national day rally?\n",
      "assistant:\u001b[0m\n",
      "2024-03-07 07:07:19 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 200, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1, 'tools': [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]}\u001b[0m\n",
      "2024-03-07 07:07:19 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=861\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:07:22 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: tool_name=gg\u001b[0m\n",
      "2024-03-07 07:07:27 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: tool_arguments={\n",
      "                \"search_query\": \"PM Lee Hsien Loong 2023 National Day Rally location\"\n",
      "            }\u001b[0m\n",
      "2024-03-07 07:07:28 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by eos_token_id: 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_05437db3-43ff-4612-8a89-666c50e409e6', function=Function(arguments='{\"search_query\": \"PM Lee Hsien Loong 2023 National Day Rally location\"}', name='gg'), type='function')])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided not to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:07:28 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: prompt=system: system:\n",
      "\n",
      "\n",
      "                You will always begin your interaction by asking yourself if the user's message is a message that requires a tool response or a text response.\n",
      "                                \n",
      "                DEFINITIONS:\n",
      "                1. A tool response is based on the following JSON format:\n",
      "                        <tool>\n",
      "                        {\n",
      "                            'function': {\n",
      "                                'name': ...,\n",
      "                                'parameters': ...\n",
      "                            }\n",
      "                        }\n",
      "                        </tool>\n",
      "                \n",
      "                And the tool is chosen from the following <tools> list:\n",
      "                        <tools>\n",
      "                        [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]\n",
      "                        </tools>.\n",
      "                    \n",
      "                2. A text response is based on the following JSON format:\n",
      "                        <text>\n",
      "                        {\n",
      "                            'text': ...\n",
      "                        }\n",
      "                        </text>\n",
      "                \n",
      "                STEPS:\n",
      "                1. Think about the nature of the user's message.\n",
      "                    * Is the user's message a question that I can answer factually within my knowledge domain?\n",
      "                    * Are there any dependencies to external factors that I need to consider before answering the user's question?\n",
      "                    * What are the tools I have at my disposal to help me answer the user's question? \n",
      "                2. If the user's message requires a tool response, pick the most suitable tool response from <tools>. \n",
      "                    * I can refer to the \"description\" field of each tool to help me decide.\n",
      "                    * For example, if I need to search for real-time information, I can use the \"gg\" tool and if I know where to find the information, I can use the \"scrape\" tool.\n",
      "                3. If the user's message does not require a tool response, provide a text response to the user.\n",
      "\n",
      "                CONSTRAINTS:        \n",
      "                1. You can only provide a tool response or a text response and nothing else.\n",
      "                2. When providing a tool response, respond only in JSON and only pick from <tools>. That means, begin your message with a curly bracket ' and end your message with a curly bracket '. Do not respond with anything else.\n",
      "                3. Remember, do not invent your own tools. You can only pick from <tools>.\n",
      "user: Tell me a one paragraph story.\n",
      "assistant:\u001b[0m\n",
      "2024-03-07 07:07:28 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 200, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1, 'tools': [{'type': 'function', 'function': {'name': 'gg', 'description': \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}, {'type': 'function', 'function': {'name': 'scrape', 'description': 'Scrape the content of the provided url', 'parameters': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The url to scrape the content from'}}, 'required': ['url']}}}]}\u001b[0m\n",
      "2024-03-07 07:07:28 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: input token count=848\u001b[0m\n",
      "2024-03-07 07:07:46 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT.streaming: stopped by eos_token_id: 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=' Once upon a time, there was a little girl named Alice who lived in an enchanted forest full of talking animals and magical creatures. One day, she stumbled upon a secret door hidden behind a waterfall and decided to explore it. As she entered through the door, she found herself in a beautiful kingdom ruled by a kind queen. The queen welcomed her warmly and showed her around the castle, introducing her to all sorts of fascinating people and things. Alice had so much fun that she never wanted to leave!', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "from gai.common.notebook import highlight\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Returning JSON\n",
    "\n",
    "Here's an example of how to return a JSON output. This is useful for returning structured from unstructured result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "The user will show you information scraped from the web. \n",
    "You will analyse and return a <relevance> format.\n",
    "\n",
    "DEFINITIONS:\n",
    "1. A <relevance> response is based on the following JSON format:\n",
    "        <relevance>\n",
    "        {\n",
    "            'type': 'json',\n",
    "            'json': {\n",
    "                'Relevance': '<relevance>%',\n",
    "                'Reason': '<reason>'\n",
    "            }\n",
    "        }\n",
    "        </relevance>\n",
    "\n",
    "You have a tendency to forget the question while you are analysing the answer. \n",
    "Follow these steps:\n",
    "1. Break down the answer into smaller chunks.\n",
    "2. Ask yourself what is the question again then assess whether any part of the information within the answer is relevant to any part of the question.\n",
    "3. Finally, you give a percentage overall score of <relevance> immediately. If it contains any direct relevance, you must give a score of at least 40%.\n",
    "You will not be given a second chance or any further context or information. If you think the answer is not related to the question, just reply with \"Not relevant\" and give a reason.\n",
    "Rule:\n",
    "a. <relevance> is a string between \"0%\" and \"100%\". If Not relevant, return \"0\".\n",
    "b. Pay attention to small part of the answer that addresses the question directly. Even when it is small part, the direct relevance is highly significant.\n",
    "c. Respond only with <relevence> response and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "My question is 'Where is PM Lee Hsien Loong 2023 National Day Rally location?'.\n",
    "Refer to this <webpage>Once upon a time, there was a little girl named Alice who lived in an enchanted forest full of talking animals and magical creatures. One day, she stumbled upon a secret door hidden behind a waterfall and decided to explore it. As she entered through the door, she found herself in a beautiful kingdom ruled by a kind queen. The queen welcomed her warmly and showed her around the castle, introducing her to all sorts of fascinating people and things. Alice had so much fun that she never wanted to leave!</webpage> and return a <relevance> response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 07:25:34 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    'type': 'json',\n",
      "    'json': {\n",
      "        'Relevance': '0%',\n",
      "        'Reason': 'The information provided does not contain any details relevant to the location of PM Lee Hsien Loong 2023 National Day Rally.'\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'system','content':system_prompt},\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        stream=True,\n",
    "        max_new_tokens=200)\n",
    "for output in response:\n",
    "    if output.choices[0].delta.content:\n",
    "        print(output.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop any container with the same name\n",
    "docker rm -f gai-ttt\n",
    "\n",
    "# Start the container\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "\n",
    "# Wait for model to load\n",
    "sleep 30\n",
    "\n",
    "# Confirm its running\n",
    "docker logs gai-ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 2: Run Text Generation Client\n",
    "\n",
    "The default model is Mistral7B-8k context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a one paragraph short story.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True)\n",
    "for chunk in response.iter_lines():\n",
    "    result = json.loads(chunk.decode('utf-8'))\n",
    "    print(result[\"choices\"][0][\"delta\"][\"content\"],end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-53e917e4-2703-4587-bede-5a5de96f12f3\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":[{\"id\":\"call_fb4edf0a-a509-4c24-91a3-78dd8b4fdcf5\",\"function\":{\"arguments\":\"{\\\"search_query\\\": \\\"current date\\\"}\",\"name\":\"gg\"},\"type\":\"function\"}]}}],\"created\":1707944860,\"model\":\"Mistral7B-ExLlama\",\"object\":\"chat.completion\",\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":3620,\"total_tokens\":3620}}\n"
     ]
    }
   ],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is today's date?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": False,\n",
    "        \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    stream=False)\n",
    "\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
